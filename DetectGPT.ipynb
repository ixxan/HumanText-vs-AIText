{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pz_-puybjg3z"},"outputs":[],"source":["# Ref:\n","# https://huggingface.co/docs/transformers/perplexity\n","# https://github.com/BurhanUlTayyab/DetectGPT,\n","# @misc{mitchell2023detectgpt,\n","#     url = {https://arxiv.org/abs/2301.11305},\n","#     author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},\n","#     title = {DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature},\n","#     publisher = {arXiv},\n","#     year = {2023},\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLd0PIMxS4Nu"},"outputs":[],"source":["!pip -q install transformers==4.26.0\n","!pip -q install sentencepiece\n","!pip -q install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_afIu4d5L_ek"},"outputs":[],"source":["import time\n","import torch\n","import itertools\n","import math\n","import numpy as np\n","import random\n","import re\n","import transformers\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","from transformers import pipeline\n","from transformers import T5Tokenizer\n","from transformers import AutoTokenizer\n","\n","from collections import OrderedDict\n","\n","from scipy.stats import norm\n","from multiprocessing.pool import ThreadPool\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","class DetectGPT:\n","    \"\"\"\n","      This model is based on Standford's DetectGPT https://arxiv.org/abs/2301.11305,\n","      which determines whether an input is generated by AI (GPT-2).\n","\n","      The model is based on the assumption that: minor rewrites of AI-generated\n","      text tend to have lower log likelihood under the model than the original sample,\n","      while minor rewrites of human-written text may have higher or lower log likelihood\n","      than the original sample.\n","\n","      The model first generates minor perturbations(rewrites) of input sample,\n","      then computes the log likelihood of the perturbation and the original sample,\n","      if the average log ratio is high, the sample is likely generated from the source model.\n","\n","      The perturbations are generated by T5-small model,\n","      then the log likelihood is scored based on GPT-2.\n","\n","      chuck_size: size of each perturbations (number of tokens)\n","      stride: number of tokens forward to move after each prediction. 1 for the most precise result,\n","              but to reduce computation time, move by larger strides.\n","      threshold: if average log ratio is greater than threshold, then it is generated by AI\n","    \"\"\"\n","    def __init__(self, device=\"cuda\", chuck_size = 20, stride = 100, threshold = 0.7):\n","        self.device = device\n","        self.score_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n","        self.score_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","\n","        self.chuck_size = chuck_size\n","        self.stride = stride\n","        self.threshold = threshold\n","\n","        self.t5_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\").to(device)\n","        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n","\n","    def run(self, sentence):\n","        sentence = re.sub(\"\\[[0-9]+\\]\", \"\", sentence)\n","\n","        words = re.split(\"[ \\n]\", sentence)\n","        groups = len(words) // self.chuck_size + 1\n","        lines = []\n","        stride = len(words) // groups + 1\n","        for i in range(0, len(words), stride):\n","            start_pos = i\n","            end_pos = min(i+stride, len(words))\n","\n","            selected_text = \" \".join(words[start_pos:end_pos])\n","            selected_text = selected_text.strip()\n","            if selected_text == \"\":\n","                continue\n","\n","            lines.append(selected_text)\n","\n","\n","        # sentence by sentence\n","        offset = \"\"\n","        scores = []\n","        probs = []\n","        labels = []\n","        for line in lines:\n","            if re.search(\"[a-zA-Z0-9]+\", line) == None:\n","                continue\n","            score, diff, sd = self.getScore(line)\n","            if score == -1 or math.isnan(score):\n","                continue\n","            scores.append(score)\n","\n","            if score > self.threshold:\n","                labels.append(1)\n","                prob = \"{:.2f}%\\n(A.I.)\".format(norm.cdf(abs(self.threshold - score)) * 100)\n","                probs.append(prob)\n","            else:\n","                labels.append(0)\n","                prob = \"{:.2f}%\\n(Human)\".format(norm.cdf(abs(self.threshold - score)) * 100)\n","                probs.append(prob)\n","\n","        mean_score = sum(scores)/len(scores)\n","        mean_prob = norm.cdf(abs(self.threshold - mean_score)) * 100\n","        label = 0 if mean_score > self.threshold else 1\n","\n","        print(\"probs: \", probs)\n","        print(\"labels: \", labels)\n","        print(\"scores: \", scores)\n","\n","        print(\"mean_prob: \", mean_prob)\n","        print(\"mean_score: \", mean_score)\n","        print(\"label: \", label)\n","\n","        print(f\"probability for {'A.I.' if label == 0 else 'Human'}:\", \"{:.2f}%\".format(mean_prob))\n","        return {\"prob\": \"{:.2f}%\".format(mean_prob), \"label\": label}, self.getVerdict(mean_score)\n","\n","    def getScore(self, sentence):\n","        original_sentence = sentence\n","        sentence_length = len(list(re.finditer(\"[^\\d\\W]+\", sentence)))\n","        #sentences = self.mask(original_sentence, original_sentence, n=50, remaining=50)\n","        sentences = self.mask(original_sentence, original_sentence)\n","\n","        real_log_likelihood = self.getLogLikelihood(original_sentence)\n","\n","        generated_log_likelihoods = []\n","        for sentence in sentences:\n","            generated_log_likelihoods.append(self.getLogLikelihood(sentence).cpu().detach().numpy())\n","\n","        if len(generated_log_likelihoods) == 0:\n","            return -1\n","\n","        generated_log_likelihoods = np.asarray(generated_log_likelihoods)\n","        mean_generated_log_likelihood = np.mean(generated_log_likelihoods)\n","        std_generated_log_likelihood = np.std(generated_log_likelihoods)\n","\n","        diff = real_log_likelihood - mean_generated_log_likelihood\n","\n","        score = diff/(std_generated_log_likelihood)\n","\n","        return float(score), float(diff), float(std_generated_log_likelihood)\n","\n","    def getLogLikelihood(self, sentence):\n","        encodings = self.score_tokenizer(sentence, return_tensors=\"pt\")\n","        seq_len = encodings.input_ids.size(1)\n","\n","        nlls = []\n","        prev_end_loc = 0\n","        for begin_loc in range(0, seq_len, self.stride):\n","            end_loc = min(begin_loc + self.score_model.config.n_positions, seq_len)\n","            trg_len = end_loc - prev_end_loc\n","            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(self.device)\n","            target_ids = input_ids.clone()\n","            target_ids[:, :-trg_len] = -100\n","\n","            with torch.no_grad():\n","                outputs = self.score_model(input_ids, labels=target_ids)\n","\n","                neg_log_likelihood = outputs.loss * trg_len\n","\n","            nlls.append(neg_log_likelihood)\n","\n","            prev_end_loc = end_loc\n","            if end_loc == seq_len:\n","                break\n","        return -1 * torch.stack(nlls).sum() / end_loc\n","\n","    def apply_extracted_fills(self, masked_texts, extracted_fills):\n","        texts = []\n","        for idx, (text, fills) in enumerate(zip(masked_texts, extracted_fills)):\n","            tokens = list(re.finditer(\"<extra_id_\\d+>\", text))\n","            if len(fills) < len(tokens):\n","                continue\n","\n","            offset = 0\n","            for fill_idx in range(len(tokens)):\n","                start, end = tokens[fill_idx].span()\n","                text = text[:start+offset] + fills[fill_idx] + text[end+offset:]\n","                offset = offset - (end - start) + len(fills[fill_idx])\n","            texts.append(text)\n","\n","        return texts\n","\n","    def unmasker(self, text, num_of_masks):\n","        num_of_masks = max(num_of_masks)\n","        stop_id = self.t5_tokenizer.encode(f\"<extra_id_{num_of_masks}>\")[0]\n","        tokens = self.t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n","        for key in tokens:\n","            tokens[key] = tokens[key].to(self.device)\n","\n","        output_sequences = self.t5_model.generate(**tokens, max_length=512, do_sample=True, top_p=0.96, num_return_sequences=1, eos_token_id=stop_id)\n","        results = self.t5_tokenizer.batch_decode(output_sequences, skip_special_tokens=False)\n","\n","        texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in results]\n","        pattern = re.compile(\"<extra_id_\\d+>\")\n","        extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n","        extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n","\n","        perturbed_texts = self.apply_extracted_fills(text, extracted_fills)\n","\n","        return perturbed_texts\n","\n","    def replaceMask(self, text, num_of_masks):\n","        with torch.no_grad():\n","            list_generated_texts = self.unmasker(text, num_of_masks)\n","\n","        return list_generated_texts\n","\n","    # code took reference from https://github.com/eric-mitchell/detect-gpt\n","    def maskRandomWord(self, text, ratio):\n","        span = 2\n","        tokens = text.split(' ')\n","        mask_string = '<<<mask>>>'\n","\n","        n_spans = ratio//(span + 2)\n","\n","        n_masks = 0\n","        while n_masks < n_spans:\n","            start = np.random.randint(0, len(tokens) - span)\n","            end = start + span\n","            search_start = max(0, start - 1)\n","            search_end = min(len(tokens), end + 1)\n","            if mask_string not in tokens[search_start:search_end]:\n","                tokens[start:end] = [mask_string]\n","                n_masks += 1\n","\n","        # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n","        num_filled = 0\n","        for idx, token in enumerate(tokens):\n","            if token == mask_string:\n","                tokens[idx] = f'<extra_id_{num_filled}>'\n","                num_filled += 1\n","        assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n","        text = ' '.join(tokens)\n","        return text, n_masks\n","\n","    def multiMaskRandomWord(self, text, ratio, n):\n","        mask_texts = []\n","        list_num_of_masks = []\n","        for i in range(n):\n","            mask_text, num_of_masks = self.maskRandomWord(text, ratio)\n","            mask_texts.append(mask_text)\n","            list_num_of_masks.append(num_of_masks)\n","        return mask_texts, list_num_of_masks\n","\n","    def getGeneratedTexts(self, args):\n","        original_text = args[0]\n","        n = args[1]\n","        texts = list(re.finditer(\"[^\\d\\W]+\", original_text))\n","        ratio = int(0.3 * len(texts))\n","\n","        mask_texts, list_num_of_masks = self.multiMaskRandomWord(original_text, ratio, n)\n","        list_generated_sentences = self.replaceMask(mask_texts, list_num_of_masks)\n","        return list_generated_sentences\n","\n","    def mask(self, original_text, text, n=50, remaining=50):\n","        \"\"\"\n","        text: string representing the sentence\n","        n: top n mask-filling to be choosen\n","        remaining: The remaining slots to be fill\n","        \"\"\"\n","\n","        if remaining <= 0:\n","            return []\n","\n","        torch.manual_seed(0)\n","        np.random.seed(0)\n","        start_time = time.time()\n","        out_sentences = []\n","        pool = ThreadPool(remaining//n)\n","        out_sentences = pool.map(self.getGeneratedTexts, [(original_text, n) for _ in range(remaining//n)])\n","        out_sentences = list(itertools.chain.from_iterable(out_sentences))\n","        end_time = time.time()\n","\n","        return out_sentences\n","\n","    def getVerdict(self, score):\n","        if score < self.threshold:\n","            return \"This text is most likely written by an Human\"\n","        else:\n","            return \"This text is most likely generated by an A.I.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cnb084Q3YwFm"},"outputs":[],"source":["detector = DetectGPT(device = \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPY-jUa1XMri"},"outputs":[],"source":["human_text = \"\"\"\n","All children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs. Darling put her hand to her heart and cried, “Oh, why can’t you remain like this for ever!” This was all that passed between them on the subject, but henceforth Wendy knew that she must grow up. You always know after you are two. Two is the beginning of the end.  Of course they lived at 14, and until Wendy came her mother was the chief one. She was a lovely lady, with a romantic mind and such a sweet mocking mouth. Her romantic mind was like the tiny boxes, one within the other, that come from the puzzling East, however many you discover there is always one more; and her sweet mocking mouth had one kiss on it that Wendy could never get, though there it was, perfectly conspicuous in the right-hand corner.  The way Mr. Darling won her was this: the many gentlemen who had been boys when she was a girl discovered simultaneously that they loved her, and they all ran to her house to propose to her except Mr. Darling, who took a cab and nipped in first, and so he got her. He got all of her, except the innermost box and the kiss. He never knew about the box, and in time he gave up trying for the kiss. Wendy thought Napoleon could have got it, but I can picture him trying, and then going off in a passion, slamming the door.  Mr. Darling used to boast to Wendy that her mother not only loved him but respected him. He was one of those deep ones who know about stocks and shares. Of course no one really knows, but he quite seemed to know, and he often said stocks were up and shares were down in a way that would have made any woman respect him.\n","\"\"\""]},{"cell_type":"code","source":["detector.run(human_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgrklWmelNms","executionInfo":{"status":"ok","timestamp":1687464708318,"user_tz":240,"elapsed":779250,"user":{"displayName":"Irpan A","userId":"04513140283106559393"}},"outputId":"297fcda1-1c73-4c5d-bcb1-4b50ba9fbe7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["probs:  ['61.60%\\n(Human)', '50.27%\\n(Human)', '58.70%\\n(Human)', '56.33%\\n(Human)', '67.32%\\n(Human)', '79.97%\\n(Human)', '68.78%\\n(Human)', '70.83%\\n(Human)', '63.30%\\n(Human)', '61.54%\\n(Human)', '71.88%\\n(Human)', '56.16%\\n(Human)', '72.78%\\n(Human)', '81.38%\\n(Human)', '59.67%\\n(A.I.)', '60.55%\\n(Human)', '71.77%\\n(Human)', '52.78%\\n(A.I.)', '61.79%\\n(A.I.)']\n","labels:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n","scores:  [0.4048844873905182, 0.6931112408638, 0.48024046421051025, 0.5405792593955994, 0.25136443972587585, -0.1406182199716568, 0.21025952696800232, 0.15155057609081268, 0.360275536775589, 0.40649646520614624, 0.12058395147323608, 0.5450475215911865, 0.09394136071205139, -0.19211921095848083, 0.9449411034584045, 0.4324142038822174, 0.12408289313316345, 0.769826352596283, 1.0]\n","mean_prob:  62.597732501520184\n","mean_score:  0.3787822080285926\n","label:  1\n","probability for Human: 62.60%\n"]},{"output_type":"execute_result","data":{"text/plain":["({'prob': '62.60%', 'label': 1},\n"," 'This text is most likely written by an Human')"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgUxSv1xZf9f"},"outputs":[],"source":["gpt3_text = \"\"\"\n","In Aldous Huxley's novel \"Brave New World,\" Mustapha Mond is portrayed as a powerful and mysterious figure. The novel depicts a dystopian society in which the government, led by Mond, maintains strict control over its citizens through the use of advanced technology and manipulation of emotions. Despite this, I argue that Mond should be viewed positively for three key reasons: his efforts to maintain stability in society, his recognition of the limitations of happiness, and his belief in individual freedom.  Firstly, Mond's role as World Controller is to maintain stability in society. He recognizes that in order for society to function, there must be a balance between individual desires and the needs of the community. He also understands that in order to maintain this balance, it is necessary to control certain aspects of society, such as the use of technology and the manipulation of emotions. This is evident in his decision to ban literature, which he believes will cause dissent and disrupt the stability of society. In this way, Mond can be seen as a pragmatic leader who is willing to make difficult decisions for the greater good.  Secondly, Mond recognizes the limitations of happiness. In the novel, the government encourages the citizens to pursue pleasure and happiness at all times, but Mond understands that this is not a sustainable or fulfilling way of life. He acknowledges that true happiness cannot be found through constant pleasure and that individuals need to find meaning and purpose in their lives. This is evident in his statement, \"ending is better than mending. The more stitches, the less riches.\" This quote shows that Mond recognizes that true happiness cannot be found in constant pleasure, but rather in finding meaning in one's life.  Lastly, Mond believes in individual freedom. Despite his role in controlling society, he recognizes that individuals have the right to make their own choices and live their lives as they see fit. This is evident in his decision to provide a reservation for those who do not want to conform to the rules of society. This shows that Mond understands that individuals should have the freedom to live their lives as they choose, even if it means going against the norms of society.  In conclusion, Mustapha Mond, the World Controller in Aldous Huxley's \"Brave New World,\" should be viewed positively for his efforts to maintain stability in society, his recognition of the limitations of happiness and his belief in individual freedom. Although his methods are controversial, they are necessary to maintain the balance of society. His recognition of the limitations of happiness and his belief in individual freedom also show that he is a nuanced and thoughtful leader who understands the complexity of human nature.\n","\"\"\""]},{"cell_type":"code","source":["detector.run(gpt3_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue-4X5GSlMRN","executionInfo":{"status":"ok","timestamp":1687414483341,"user_tz":240,"elapsed":258113,"user":{"displayName":"Irpan A","userId":"04513140283106559393"}},"outputId":"012981ee-c9a1-4c5f-cb52-833af3ad4265"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["probs:  ['50.86%\\n(A.I.)', '65.41%\\n(Human)', '76.18%\\n(Human)', '60.78%\\n(Human)', '88.45%\\n(Human)', '62.19%\\n(Human)', '54.21%\\n(A.I.)', '54.91%\\n(Human)', '74.74%\\n(Human)', '69.41%\\n(Human)', '83.73%\\n(Human)', '58.54%\\n(Human)', '51.61%\\n(A.I.)', '69.59%\\n(Human)', '75.56%\\n(Human)', '65.85%\\n(Human)', '53.05%\\n(A.I.)', '70.88%\\n(Human)', '63.61%\\n(Human)', '56.11%\\n(Human)', '62.52%\\n(Human)', '56.67%\\n(Human)', '62.64%\\n(A.I.)']\n","labels:  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n","scores:  [0.7215920686721802, 0.3034645617008209, -0.012062099762260914, 0.42642199993133545, -0.4977119266986847, 0.38957303762435913, 0.8056710362434387, 0.5766503214836121, 0.033785343170166016, 0.192595973610878, -0.28360387682914734, 0.48418810963630676, 0.7403052449226379, 0.1873776614665985, 0.0076712737791240215, 0.2915721833705902, 0.7764610648155212, 0.1501644253730774, 0.35187166929244995, 0.5461698770523071, 0.38077831268310547, 0.5321094989776611, 1.0224411487579346]\n","mean_prob:  63.556572469758144\n","mean_score:  0.3533689960553918\n","label:  1\n","probability for Human: 63.56%\n"]},{"output_type":"execute_result","data":{"text/plain":["({'prob': '63.56%', 'label': 1},\n"," 'This text is most likely written by an Human')"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["gpt2_text = \"\"\"\n","The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.  Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.  Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.  Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.  Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.  While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”  Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.  While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”  However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n","\"\"\""],"metadata":{"id":"ZZ8gI2M-lZdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detector.run(gpt2_text)"],"metadata":{"id":"8s5WPK3sMwlk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687457598970,"user_tz":240,"elapsed":1300423,"user":{"displayName":"Irpan A","userId":"04513140283106559393"}},"outputId":"f8defe7f-feb7-4803-85d3-de56ae6a2083"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["probs:  ['80.64%\\n(Human)', '66.05%\\n(Human)', '76.69%\\n(Human)', '75.90%\\n(Human)', '76.90%\\n(Human)', '78.77%\\n(Human)', '75.26%\\n(Human)', '51.65%\\n(A.I.)', '80.77%\\n(Human)', '72.58%\\n(Human)', '65.50%\\n(Human)', '58.80%\\n(A.I.)', '73.70%\\n(Human)', '61.17%\\n(Human)', '67.95%\\n(Human)', '62.27%\\n(Human)', '62.47%\\n(A.I.)', '61.79%\\n(A.I.)']\n","labels:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n","scores:  [-0.1647581160068512, 0.2862550914287567, -0.028605831786990166, -0.003081496339291334, -0.03552785888314247, -0.0983869805932045, 0.017453771084547043, 0.7414236068725586, -0.16943992674350739, 0.09976842254400253, 0.3012216091156006, 0.9224386215209961, 0.06602905690670013, 0.4162788689136505, 0.2337300181388855, 0.3874930441379547, 1.017975926399231, 1.0]\n","mean_prob:  66.3765870254372\n","mean_score:  0.27723710148388314\n","label:  1\n","probability for Human: 66.38%\n"]},{"output_type":"execute_result","data":{"text/plain":["({'prob': '66.38%', 'label': 1},\n"," 'This text is most likely written by an Human')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["detector.run(gpt2_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48zs31Ah159x","executionInfo":{"status":"ok","timestamp":1687450112247,"user_tz":240,"elapsed":326436,"user":{"displayName":"Irpan A","userId":"04513140283106559393"}},"outputId":"c3196d3c-de57-485f-b82c-2d6c84437f89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["probs:  ['55.41%\\n(A.I.)', '58.75%\\n(Human)', '63.61%\\n(Human)', '56.05%\\n(Human)', '65.73%\\n(Human)', '75.34%\\n(Human)', '61.79%\\n(A.I.)']\n","labels:  [1, 0, 0, 0, 0, 0, 1]\n","scores:  [0.836114764213562, 0.4789677560329437, 0.35201725363731384, 0.5477941036224365, 0.2949652075767517, 0.01479245349764824, 1.0]\n","mean_prob:  57.78821123364709\n","mean_score:  0.5035216483686652\n","label:  1\n","probability for Human: 57.79%\n"]},{"output_type":"execute_result","data":{"text/plain":["({'prob': '57.79%', 'label': 1},\n"," 'This text is most likely written by an Human')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"_S3UcCQB1-6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"re8i998clZUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OI7HIAfulZRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwtLUz7dfI3s"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmkl5abuhbJ5+LpLHRYBw/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}